Composed Minimum Viable Product (MVP) for Core User Demographic “A”	

I. Product Flow and U/X	

Argument 1. A user journey begins with low -friction registration designed to be conversational and to keep the user inside the app. [The user does not leave the app to authenticate via email.]	

Argument 2. Upon opening the app, the user selects “register”, or “log-in”. If the user is registering, she or he inputs values in the name, gender, and age fields. Then she or he receives a unique authentication token.	

Argument 3. After the user registers, she or he will be prompted to select a color-coded emoji face from one of five emoji faces; the colors and expressions represent the vectorspace of healthy states on both the Likert scale and an affective scale, with the polar states on either side of the scale, and the most neutral states, beginning from left to right, increasing in valence, and dominance towards the right side of the scale, and increasing in arousal and dominance on the left side of the scale.	

<<API library: MyFunction, “I think I understand. Could you tell me more, in just a few words?”	5
<< “auto-prompt, print: “I feel”	

<< If the user times-out without inputting, or does not complete an input	

<<“Sometimes I’m shy…Sometimes I don’t know how I feel.”	
<<reponse “ANOVA of variance between affective scale response and absence of response”	

<<Else, API library, MyFunction: See “NLP” model and algorithms for the proposed text-based search process. “NLP” model and algorithms for the proposed text-based search process.	

Argument 4. The user is then presented with the choice of the four use cases (relaxation, exercise, sleep, and mood boosting).	

API library, MyFunction: See equations for use cases.	

a. The user taps on the button for the use case	

API library, MyFunction: See equations for use cases.	

b.  A 30 minute listening session starts.	

Argument 5. The user concludes their listening session and is prompted to select the emotional state that best represents their emotional state [after having concluded their 30 minute music listening session]. They must select only one emoji face from amongst five emoji faces, ranging, from left to right, from least to greatest positive [valence]. This concludes the music listening session.	


    
I. Product Flow and U/X

	Argument 1. A user journey begins with low -friction registration designed to be conversational and to keep the user inside the app. [The user does not leave the app to authenticate via email.]

	API library, MyFunction: See “chatbot registration” for the proposed method.  Please see “NLP” functions on polarity and EIF of natural language inputs generated by the user.

	Argument 2. Upon opening the app, the user selects “register”, or “log-in”. If the user is registering, she or he inputs values in the name, gender, and age fields. Then she or he receives a unique authentication token. 
      
	API library, MyFunction:  See “webauthen” for the proposed secured access, passwordless registration method. 

	Input: constraints, [print: last, first] 

		<< “It’s nice to meet you! My name’s Aiia.”

	Else, [print: first]

		<< “It’s nice to meet you! My name’s Aiia.”
	
	Else, [no response before time-out, or “enter” or “shift” key input, proceed to gender]

		<< “Do you have a gender? If so, do you see it in this list?”
			
		<< print: 00 Gender Inclusive; 01 gender fluid; 11 feminine; 10 masculine.
	

	Else, [no response before time-out, or “enter” or “shift” key input, proceed to age]

	<< “I was born in 2008. What about you?”

 	<< print:  myFunction, [45-60 years], “I bet you take vitamins! 
	
		   myFunction, [35-45 years], “Awesome!”

	Else, [no response before time-out, or “enter” or “shift” key input, proceed to 		disclaimer:

		
	Argument 3. After the user registers, she or he will be prompted to select a color-coded emoji face from one of five emoji faces; the colors and expressions represent the vectorspace of healthy states on both the Likert scale and an affective scale, with the polar states on either side of the scale, and the most neutral states, beginning from left to right, increasing in valence, and dominance towards the right side of the scale, and increasing in arousal and dominance on the left side of the scale. 

		<<API library: MyFunction, “I think I understand. Could you tell me more, in just a few words?”
		

		<< “auto-prompt, print: “I feel”

		<< If the user times-out without inputting, or does not complete an input

			<<“Sometimes I’m shy…Sometimes I don’t know how I feel.”				
				<<reponse “ANOVA of variance between affective scale response and absence of response”

		<<Else, API library, MyFunction: See “NLP” model and algorithms for the proposed text-based search process. “NLP” model and algorithms for the proposed text-based search process.
	

			<<API library, MyFunction: See Bayesian probability model, lyric bag of words scraped from million song dataset and own music compositions.

		<< pre-programmed “smart phrases” auto-fill as the user enters inputs; slower inputs auto-prompt stronger polarity in response

			
			<<API library, MyFunction: See “auto-fill functions” for the proposed methods for predictive, “smart text” prompts and auto-fill features.
	

		<<pre-programmed responses scraped from demographic-appropriate NLP dataset

			<<API library, MyFunction: See “distance functions” for the proposed methods to ascertain the nearest and furthest emotional states to the state prompted by the user.
			

	Argument 4. The user is then presented with the choice of the four use cases (relaxation, exercise, sleep, and mood boosting). 
	
	API library, MyFunction: See equations for use cases. 

	    a. The user taps on the button for the use case

			API library, MyFunction: See equations for use cases. 

			
	    b.  A 30 minute listening session starts. 

			API library, MyFunction: See “playlist” for the proposed methods for dynamic, real-time, “conversational” playlist creation.
			API library, MyFunction: See “library” for more information on the compositions contained in the music library for the MVP. 

	Argument 5. The user concludes their listening session and is prompted to select the emotional state that best represents their emotional state [after having concluded their 30 minute music listening session]. They must select only one emoji face from amongst five emoji faces, ranging, from left to right, from least to greatest positive [valence]. This concludes the music listening session. 



Required Prior to User [Journey] 1

Modeling A Population’s EIF 
	<< WTH would we do this? Print: neuropersonalization
		“What’s the healthy range of emotional states for our customer’s users?”
		“We know these user listen to a lot of music, but: are they plugging in and zoning out, having a living room dance party, or needing motivation for deadlifts, or an outside run or walk? Do they have a touch of insomnia?”
		“How self-aware are they? How comfortable are they with examining their emotional and physical state?” 

	<<Iff the population represents a highly-defined and narrow user population with a critical mass of users; [PCA, KNN, CNN, RL, statistical modeling; mixed methods] this method [is used] are used to select fine grain detail on user population’s emotions, statistical learning capacity, and musical expectations; 

		<<Else, to solve for x, y, and z given necessary data on user population for affective feature extraction, [IE emotional state feature extraction [emotional state and use case predictive modeling] 
			[and linear regressive modeling [to examine ANOVA for similar analyses of emotional states for comparable populations]
			[using time and time frequency domain analysis]
			 [of the same population segment emotional state validation during 1991, 2001, and 2008, as applicable, based on age, for hypothesis validation]. 

		<<Else, use a convolutional neural network trained on intended inputs and outputs for the given emotional states and use cases

			<<Else, use a Bayesian probability model and Decision Tree to factor all possible inputs and outputs given the number of use cases, the number of compositions in the music library, and a user group of 10 [simulated] users

			<< Else, to construct a decision tree that randomizes registration inputs, given a user’s refusal to input as prompted
			<< Else, that models a proposed user, for interactive demo with customer
				<< Else, that demonstrates the dynamic affective state assessment [input and output process] [only after registration or sign-in]


		<<Below, explanation of the affective scales and Likert scale [for application], to sign-post the application of associated API library [myFunctions, etc] in context of [user emotional assessment, proprietary affective scales, and use case simulation]
			
A. A Narrow User Population Segment 
         1. Selected using Bayesian Probability
		   2.  Selected based on streaming and radio user 
			listening behavior 

			a. Selected based on intended premium audience

			    1. In radio, based on demographic with provable highest ROI on 
				advertising and marketing spend
	B. Emotional states and use cases derived from rectilinear selections representing the 	
	   ANOVA [of emotional states] for healthy members of this population segment
		1. Based on NLP correlates for the healthy 10 point range on “Form Y” of the 				STAI 
		2. Based on Affective Scale correlates in our EIF circumplex 4 dimensional 				model of emotion
		3. Based on the Likert scale
			a. A Mixed-Methods Bayesian 
		           	Probability Model 		
    	C. Comparatively Small Rectilinear Spaces of Emotion, Affect 	
			a. With non-polar boundaries
			b.   With variables that do not intersect with the x, y, or z axes
			c.   With coordinates that represent the closest possible to the inverse 
			      of the coordinates for the  x and y coordinates for the state at the 
			      furthest point on the Likert scale, ranging from 1 to 5, in only 	
			      integers, with the state in the center being comprised of a mixed 
			      emotional state that, left of the center of the emoji is higher in 
			      arousal, and right of the center of the emoji, is higher in valence
	D. As above, for music library selection



In Order to Generate A Fixed Point [Pre-Programmed] Plot of All Possible Emotional States 
	<<Iff the fixed point plot is comprised of points whose coordinates (x, y, z) within the pre-determined rectilinear space
	<<IFF the fixed point plot is comprised of points whose coordinates representing all emotional states both explicitly and implicitly included in the affective measurement scales
	<<IFF, the fixed point plot is comprised of points whose coordinates adhere to “myFunction, x and y integer values, determined using non-recursive, simplified formulations of the Fibonacci sequence, the Binet Numbers, the Golden Ratio, the Silver Ratio, etc [phi, Phi, and all cosPhi values, etc] 
	<<IFF, the distance between any two states is measurable

	<<IFF, the time needed to travel the distance between any two emotional states can be measured, given the constant z1, the total time a given user would need to listen to all compositions for any given vector in the rectilinear space; [4th dimensional] this interpolant [the dominance of a musical composition] validates the availability of compositions for the given use cases, for the given number of simulated users

 		<< As above, depending on the size of the music library, PCA, KNN, CNN, RL, statistical modeling; mixed methods] this method [is used] are used to select fine grain detail on user population’s emotions, statistical learning capacity, and musical expectations;

	<< In order to identify mixed emotional states,

	<< In order to identify the distance between mixed emotional states and emotional states that adhere to the above criteria, a self organizing map is utilized

	<< The coordinates [neurons] in the [rectilinear space] net [neighborhood] plot, or [self-organize] according to their emotional proximity [in the neighborhood]. 
	<< API library; see [computes dominance]; any given neuron’s [self-organized] coordinates [valence, arousal, and dominance] can be measured such that their distance [vector-space] is the average 
		<<input, given an output of 1 		<<input, given an output of the furthest [vector] from the any selected [neuron’s] coordinates
		

<<SOMVADInputs - input data.

<<inputs = VADInputs;

<<Create VAD Neighborhood Map

	dimension1 = 10;
	dimension2 = 10;
	net = selforgmap([dimension1 dimension2]);

<<% Train [the neural net]
	[net,tr] = train(net,inputs);

<< % Test [the neural net]
	outputs = net(inputs);

	% Extract Plot [the neural net]
	view(net)







